# -*- coding: utf-8 -*-
"""Modelo01_v1_01.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wGOoZWcYAkRYm-Vu8YlOtKE_IC3THhPs
"""

# ==========================
# REPRODUCIBILIDAD Y UMBRAL
# ==========================
# (1) Determinismo fuerte
import os, random
os.environ["PYTHONHASHSEED"] = "0"
os.environ["TF_DETERMINISTIC_OPS"] = "1"
os.environ["TF_CUDNN_DETERMINISTIC"] = "1"

import numpy as np
random.seed(0)
np.random.seed(0)

#  importa TensorFlow y configura hilos
import tensorflow as tf
tf.random.set_seed(0)
tf.config.threading.set_inter_op_parallelism_threads(1)
tf.config.threading.set_intra_op_parallelism_threads(1)

# ==========================
# LIBRERÍAS (set original)
# ==========================
import pandas as pd
import matplotlib.pyplot as plt
import io
import seaborn as sns
from google.colab import files

from sklearn.preprocessing import RobustScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, RepeatVector, TimeDistributed
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score

print("Semillas fijadas y ops deterministas activadas.")

# ==========================
# Bloque 0: subido de archivo
# ==========================
print("Sube el archivo NORMAL (ej: 'WindTurbine_...csv')")
uploaded_normal = files.upload()
file_normal = list(uploaded_normal.keys())[0]
print(f"Listo: {file_normal}")

print("\nSube el archivo con eventos (ej: 'Train_...csv')")
uploaded_anom = files.upload()
file_anom = list(uploaded_anom.keys())[0]
print(f"Listo: {file_anom}")

# Parámetros
TIME_STEPS = 100  # tamaño de la ventana para el LSTM

# --- Función para armar secuencias 3D (samples, time_steps, features) ---
def crear_secuencias(data_2d, time_steps):
    X = []
    for i in range(len(data_2d) - time_steps):
        X.append(data_2d[i:(i + time_steps)])
    return np.array(X)

# ==========================
# 1) Leo CSV normal, ordeno, columnas
# ==========================
df_norm = pd.read_csv(io.BytesIO(uploaded_normal[file_normal]), low_memory=False)
df_norm = df_norm.sort_values("timestamp").reset_index(drop=True)

cols_features = ["amplitud_max"] + [f"dB_{i}Hz" for i in range(1, 21)]
cols_features = [c for c in cols_features if c in df_norm.columns]

X_norm_all = df_norm[cols_features].copy()
print("filas normales totales:", len(X_norm_all))
print("columnas usadas:", cols_features)

# ==========================
# 2) Split temporal: 70%/30%
# ==========================
n = len(X_norm_all)
i_train = int(n * 0.70)
X_train_df = X_norm_all.iloc[:i_train].copy()
X_val_df   = X_norm_all.iloc[i_train:].copy()

# quitar columnas constantes (IQR=0) en TRAIN
iqr = (X_train_df.quantile(0.75) - X_train_df.quantile(0.25))
const_cols = list(iqr[iqr == 0].index)
if const_cols:
    print("Columnas constantes en TRAIN (las quito):", const_cols)
    X_train_df = X_train_df.drop(columns=const_cols)
    X_val_df   = X_val_df.drop(columns=const_cols)
    cols_features = [c for c in cols_features if c not in const_cols]

# ==========================
# 3) Escalado robusto
# ==========================
scaler = RobustScaler()
X_train_s = scaler.fit_transform(X_train_df)
X_val_s   = scaler.transform(X_val_df)

# ==========================
# 4) Secuencias
# ==========================
X_train = crear_secuencias(X_train_s, TIME_STEPS)
X_val   = crear_secuencias(X_val_s, TIME_STEPS)
y_train = X_train
y_val   = X_val

print("shape X_train:", X_train.shape)
print("shape X_val:  ", X_val.shape)

# ==========================
# 5) Autoencoder LSTM (tanh)
#    + Limpio sesión para evitar estado previo
# ==========================
tf.keras.backend.clear_session()
n_features = X_train.shape[2]

model = Sequential()
# encoder
model.add(LSTM(64, activation='tanh', return_sequences=True, input_shape=(TIME_STEPS, n_features)))
model.add(LSTM(16, activation='tanh', return_sequences=False))
# decoder
model.add(RepeatVector(TIME_STEPS))
model.add(LSTM(16, activation='tanh', return_sequences=True))
model.add(LSTM(64, activation='tanh', return_sequences=True))
model.add(TimeDistributed(Dense(n_features)))

model.compile(optimizer='adam', loss='mae')
model.summary()

es  = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
rlr = ReduceLROnPlateau(monitor='val_loss', patience=5, factor=0.5, verbose=1)

hist = model.fit(
    X_train, y_train,
    epochs=100,
    batch_size=64,
    validation_data=(X_val, y_val),
    shuffle=False,
    callbacks=[es, rlr],
    verbose=1
)

# ==========================
# Curva de pérdida
# ==========================
plt.figure()
plt.plot(hist.history['loss'], label='entrenamiento')
plt.plot(hist.history['val_loss'], label='validación')
plt.legend()
plt.title('pérdida durante el entrenamiento')
plt.xlabel('época')
plt.ylabel('MAE')
plt.tight_layout()
plt.savefig('training_loss.png')
plt.close()
print("gráfico 'training_loss.png' guardado.")

# ==========================
# 6) Umbral (estable con fallback a archivo)
# ==========================
# Intento cargar un umbral fijo de una corrida previa, si existe:
threshold_file = "threshold.txt"
threshold = None
if os.path.exists(threshold_file):
    try:
        with open(threshold_file, "r") as f:
            threshold = float(f.read().strip())
        print(f"Umbral cargado desde '{threshold_file}': {threshold}")
    except:
        threshold = None

# Si no existe, lo calculo (p99.5) y LO GUARDO para las próximas corridas
X_val_pred = model.predict(X_val, verbose=0)
val_mae = np.mean(np.abs(X_val_pred - X_val), axis=(1, 2))

if threshold is None:
    threshold = np.quantile(val_mae, 0.995)
    with open(threshold_file, "w") as f:
        f.write(str(threshold))
    print("Umbral calculado (p99.5 en validación) y guardado en 'threshold.txt':", threshold)
else:
    print("Usando umbral fijo previamente guardado.")

plt.figure()
plt.hist(val_mae, bins=50)
plt.axvline(threshold, linestyle='--')
plt.title('distribución del error (validación)')
plt.xlabel('MAE por secuencia')
plt.ylabel('frecuencia')
plt.tight_layout()
plt.savefig('error_val_hist.png')
plt.close()
print("gráfico 'error_val_hist.png' guardado.")

# ==========================
# 7) Proceso para anomalías
# ==========================
df_anom = pd.read_csv(io.BytesIO(uploaded_anom[file_anom]), low_memory=False)
df_anom = df_anom.sort_values("timestamp").reset_index(drop=True)

cols_comunes = [c for c in cols_features if c in df_anom.columns]
if set(cols_features) - set(cols_comunes):
    print("alerta: faltan columnas en el archivo de anomalías:", list(set(cols_features) - set(cols_comunes)))

X_anom_df = df_anom[cols_comunes].copy()
X_anom_s = scaler.transform(X_anom_df)

X_anom = crear_secuencias(X_anom_s, TIME_STEPS)
X_anom_pred = model.predict(X_anom, verbose=0)
anom_mae = np.mean(np.abs(X_anom_pred - X_anom), axis=(1, 2))

flags = anom_mae > threshold
print(f"secuencias anómalas detectadas: {np.sum(flags)} de {len(flags)}")

plt.figure()
plt.hist(anom_mae, bins=50)
plt.axvline(threshold, linestyle='--')
plt.title('errores en archivo de anomalías (comparado con el umbral)')
plt.xlabel('MAE por secuencia')
plt.ylabel('frecuencia')
plt.tight_layout()
plt.savefig('error_anom_hist.png')
plt.close()
print("gráfico 'error_anom_hist.png' guardado.")

#  Métricas de eficacia (evaluo con VALIDACIÓN vs ANOMALÍAS)


# En este punto ya tengo:
# - val_mae: errores MAE de validación (normal)
# - anom_mae: errores MAE del archivo de anomalías
# - threshold: umbral elegido (p99.5 de val_mae)

# 1) Armo las etiquetas verdaderas (0 = normal, 1 = anómalo)
y_true = np.concatenate([
    np.zeros(len(val_mae), dtype=int),   # todo esto es normal
    np.ones(len(anom_mae), dtype=int)    # esto es anómalo
])

# 2) Convierto los errores en predicciones binarias usando el umbral
#    si el error pasa el umbral => anómalo (1), si no => normal (0)
pred_norm  = (val_mae  > threshold).astype(int)
pred_anom  = (anom_mae > threshold).astype(int)
y_pred = np.concatenate([pred_norm, pred_anom])

# 3) También preparo un vector de "scores" continuo para AUROC/AUPRC
#    uso el MAE tal cual como "score de anomalía": mayor MAE = más anómalo
y_score = np.concatenate([val_mae, anom_mae])

# 4) Matriz de confusión
cm = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Pred. Normal', 'Pred. Anómalo'],
            yticklabels=['Real Normal', 'Real Anómalo'])
plt.title('Matriz de confusión (val vs anom)')
plt.xlabel('Predicción')
plt.ylabel('Real')
plt.tight_layout()
plt.savefig('confusion_matrix.png')
plt.show()
print("Gráfico 'confusion_matrix.png' guardado.")

# 5) Métricas con el umbral seleccionado
precision = precision_score(y_true, y_pred, zero_division=0)
recall    = recall_score(y_true, y_pred, zero_division=0)
f1        = f1_score(y_true, y_pred, zero_division=0)

print("\n--- Métricas con el umbral elegido ---")
print(f"Precisión: {precision:.4f}")
print(f"Sensibilidad (Recall): {recall:.4f}")
print(f"F1-Score: {f1:.4f}")

# 6) Métricas globales sin umbral
#    AUROC y AUPRC usan el score continuo (MAE)
try:
    auroc = roc_auc_score(y_true, y_score)
    auprc = average_precision_score(y_true, y_score)
    print("\n--- Métricas sin umbral ---")
    print(f"AUROC (ROC AUC): {auroc:.4f}")
    print(f"AUPRC (PR AUC): {auprc:.4f}")
except Exception as e:
    print("\nNo pude calcular AUROC/AUPRC:", e)

# 7)  Resumen de “falsos positivos sobre validación”
fp = cm[0,1]  # normales que marcó como anómalas
tn = cm[0,0]
fn = cm[1,0]
tp = cm[1,1]
print(f"\nResumen rápido:")
print(f"  Falsos positivos en validación (normales marcados como anómalos): {fp}")
print(f"  Falsos negativos en anomalías (anómalos marcados como normales): {fn}")

"""---

# Modelo **clasificador**

C Basic 28 agosto 2025

# **Guardar Tranformaciones de datos y Modelo Entrenado**
La idea es guardarlos y compartirlos en otros software
"""

# =========================================================
# TABLA + MÉTRICAS: Subir CSV → Eventos → Severidad/ABCD → Clasificación
# =========================================================
import os, numpy as np, pandas as pd, tensorflow as tf

# ---------- Requisitos del AE (ya corrido) ----------
_required = ['model','scaler','cols_features','TIME_STEPS','val_mae','df_norm','threshold','crear_secuencias']
_missing  = [v for v in _required if v not in globals()]
if _missing:
    raise RuntimeError(f"Faltan variables del AE: {_missing}. Ejecuta primero tu bloque del autoencoder.")

# ---------- Parámetros ----------
N_CONSEC = 3     # ventanas consecutivas para formar evento
GAP_S    = 10.0  # unir bloques con huecos < GAP_S
RANDOM_SEED = 0

# ---------- Utilidades ----------
def _parse_timestamp_series(s: pd.Series):
    """
    Devuelve (t_datetime, dt_s, modo):
      - t_datetime: serie datetime si es plausible (>= 2000-01-01), si no None
      - dt_s: paso temporal medio en segundos (robusto)
      - modo: 'abs' (absoluto), 'rel' (relativo), 'none' (no se pudo)
    """
    try:
        t = pd.to_datetime(s, errors='coerce')
        if not t.isna().all():
            diffs = t.astype('int64').diff().dropna().values.astype(np.float64)
            dt_s = float(np.median(diffs)/1e9) if len(diffs)>0 else 1.0
            if t.dropna().iloc[0] >= pd.Timestamp('2000-01-01'):
                return (t, dt_s if dt_s>0 else 1.0, 'abs')
    except Exception:
        pass
    try:
        x = pd.to_numeric(s, errors='coerce')
        if x.isna().all(): return (None, 1.0, 'none')
        dif = np.diff(x.dropna().values)
        dt_s = float(np.median(dif)) if len(dif)>0 else 1.0
        # heurística de unidades largas (ms, ns): normalizo a segundos
        xmax = float(np.nanmax(x.values))
        if xmax > 1e12:   dt_s /= 1e9  # ns → s
        elif xmax > 1e10: dt_s /= 1e3  # ms → s
        return (None, dt_s if dt_s>0 else 1.0, 'rel')
    except Exception:
        return (None, 1.0, 'none')

def _band_cols(cols, lo=(1,5), mid=(5,15)):
    lo_cols = [f"dB_{i}Hz" for i in range(lo[0], lo[1]+1) if f"dB_{i}Hz" in cols]
    mid_cols= [f"dB_{i}Hz" for i in range(mid[0], mid[1]+1) if f"dB_{i}Hz" in cols]
    return lo_cols, mid_cols

def mae_por_secuencia(ae: tf.keras.Model, X_seq: np.ndarray) -> np.ndarray:
    X_pred = ae.predict(X_seq, verbose=0)
    return np.mean(np.abs(X_pred - X_seq), axis=(1, 2))

def flags_anomalia(mae_seq: np.ndarray, umbral: float, n_consec: int=3) -> np.ndarray:
    base = (mae_seq > umbral).astype(int)
    if n_consec<=1: return base.astype(bool)
    runs = np.convolve(base, np.ones(n_consec, dtype=int), mode='same')
    return (runs >= n_consec)

def agrupar_eventos(flags: np.ndarray, dt_s: float, gap_s: float=10.0):
    idx = np.where(flags)[0]
    if len(idx)==0: return []
    evs, s, p = [], idx[0], idx[0]
    for i in idx[1:]:
        gap = i - p - 1
        if gap>0 and gap*dt_s >= gap_s:
            evs.append((s, p)); s = i
        p = i
    evs.append((s, p))
    return evs

def _seq_to_rows(seq_i: int, seq_j: int, time_steps: int):
    return seq_i, seq_j + time_steps - 1

def stats_normalidad(df_normal: pd.DataFrame):
    ref={}
    if 'amplitud_max' in df_normal.columns:
        mu, sd = float(df_normal['amplitud_max'].mean()), float(df_normal['amplitud_max'].std() or 1e-6)
        ref['amplitud_max']={'mu':mu,'sd':sd if sd>0 else 1e-6}
    lo_cols,_ = _band_cols(df_normal.columns)
    if len(lo_cols)>0:
        v = df_normal[lo_cols].mean(axis=1)
        mu, sd = float(v.mean()), float(v.std() or 1e-6)
        ref['ener_low_prom']={'mu':mu,'sd':sd if sd>0 else 1e-6}
    return ref

def severidad_score(z_mae, z_amp, z_low, dur_s):
    # Pesos + cortes más conservadores
    S = 0.30*z_mae + 0.25*z_amp + 0.20*z_low + 0.25*(dur_s/20.0)
    S = float(np.clip(S, -3, 8))
    if S < 1.5: sev = 1
    elif S < 3.0: sev = 2
    elif S < 4.5: sev = 3
    else: sev = 4
    return S, sev

def decidir_ABCD(tipo: str, severidad: int,
                 pico_amp: float,
                 ref_amp_p99: float = None,
                 ref_amp_p99_9: float = None,
                 z_low: float = 0.0) -> str:
    """
    D: solo si severidad=4 y (tipo=='sismo' o pico_amp≥p99.9 o z_low≥5)
    C: severidad=4 (no crítico) o severidad=3
    B: severidad=2
    A: severidad=1
    """
    t = (tipo or '').lower()
    critico = (t == 'sismo')
    limite_duro = ((ref_amp_p99_9 is not None and pico_amp >= ref_amp_p99_9) or (z_low >= 5.0))
    if severidad == 4 and (critico or limite_duro): return 'D'
    if severidad >= 3: return 'C'
    if severidad == 2: return 'B'
    return 'A'

# ---------- Subida de CSVs ----------
try:
    from google.colab import files
except Exception as e:
    raise RuntimeError("Este bloque de subida requiere Google Colab.") from e

print("🔼 Selecciona UNO o VARIOS CSV (Sismo, Tren, Sirenas, Truenos, Helicóptero, etc.)")
_uploaded = files.upload()
paths_csv = []
for name, data in _uploaded.items():
    with open(name, "wb") as f:
        f.write(data)
    paths_csv.append(name)
print("Archivos cargados:", [os.path.basename(p) for p in paths_csv])

# ---------- Referencias de normalidad ----------
ref_norm     = stats_normalidad(df_norm)
val_mae_mu   = float(np.mean(val_mae))
val_mae_sd   = float(np.std(val_mae) or 1e-6)
ref_amp_p99  = float(np.percentile(df_norm['amplitud_max'].values, 99.0))  if 'amplitud_max' in df_norm.columns else None
ref_p99_9    = float(np.percentile(df_norm['amplitud_max'].values, 99.9)) if 'amplitud_max' in df_norm.columns else None

# ---------- MÉTRICAS del detector sobre NORMALIDAD ----------
# Usamos val_mae (ventanas de normalidad) para estimar FPR por ventana y falsos eventos
fpr_windows = float((val_mae > threshold).mean()) if isinstance(val_mae, np.ndarray) and len(val_mae)>0 else np.nan
# Duración promedio de una ventana ~ dt_s (estimado de df_norm)
_, dt_norm, _ = _parse_timestamp_series(df_norm['timestamp']) if 'timestamp' in df_norm.columns else (None, 1.0, 'none')
flags_norm   = flags_anomalia(val_mae, umbral=threshold, n_consec=N_CONSEC) if isinstance(val_mae, np.ndarray) else np.array([])
evs_norm     = agrupar_eventos(flags_norm, dt_norm, gap_s=GAP_S) if len(flags_norm)>0 else []
print("\n=== DETECTOR (sanidad sobre normalidad) ===")
print(f"FPR por ventana (normalidad): {fpr_windows:.5f}")
print(f"Falsos eventos en normalidad (N_CONSEC={N_CONSEC}, GAP={GAP_S}s): {len(evs_norm)}")

# ---------- Construcción de eventos y features (para todos los CSV) ----------
def _tipo_por_nombre(base: str) -> str:
    b = base.lower()
    if   'sismo' in b: return 'sismo'
    if   'train' in b or 'tren' in b: return 'tren'
    if   'sirena' in b: return 'sirena'
    if   'trueno' in b or 'thunder' in b: return 'trueno'
    if   'helicop' in b: return 'helicoptero'
    return 'otros'

rows = []
for path_csv in paths_csv:
    base = os.path.splitext(os.path.basename(path_csv))[0]
    tipo_hint = _tipo_por_nombre(base)
    df = pd.read_csv(path_csv).sort_values("timestamp").reset_index(drop=True)

    cols_comunes = [c for c in cols_features if c in df.columns]
    if len(cols_comunes)==0:
        print(f"[WARN] {os.path.basename(path_csv)} sin columnas comunes con {cols_features}; salto.")
        continue

    t_parsed, dt_s, modo_t = _parse_timestamp_series(df['timestamp']) if 'timestamp' in df.columns else (None, 1.0, 'none')

    Xs = scaler.transform(df[cols_comunes].copy())
    X_seq = crear_secuencias(Xs, TIME_STEPS)
    if len(X_seq)==0:
        print(f"[WARN] {os.path.basename(path_csv)}: no se formaron secuencias.")
        continue
    mae_seq = mae_por_secuencia(model, X_seq)

    flags = flags_anomalia(mae_seq, umbral=threshold, n_consec=N_CONSEC)
    evs   = agrupar_eventos(flags, dt_s, gap_s=GAP_S)

    lo_cols, _ = _band_cols(df.columns)
    for k,(s,e) in enumerate(evs,1):
        r0,r1 = _seq_to_rows(s,e,TIME_STEPS); r1 = min(r1,len(df)-1)
        nfil  = max(1, r1-r0+1)
        dur_s = nfil * dt_s
        v_mae = mae_seq[s:e+1]
        mae_pico = float(np.max(v_mae)); mae_prom = float(np.mean(v_mae))
        amp_pico = float(df['amplitud_max'].iloc[r0:r1+1].max()) if 'amplitud_max' in df.columns else np.nan
        ener_low_prom = float(df[lo_cols].iloc[r0:r1+1].mean().mean()) if len(lo_cols)>0 else np.nan

        # z-scores para severidad
        z_mae = (mae_pico - val_mae_mu)/val_mae_sd
        if 'amplitud_max' in ref_norm:
            muA, sdA = ref_norm['amplitud_max']['mu'], ref_norm['amplitud_max']['sd']
            z_amp = (amp_pico - muA)/(sdA if sdA>0 else 1e-6)
        else:
            z_amp = 0.0
        if 'ener_low_prom' in ref_norm and not np.isnan(ener_low_prom):
            muL, sdL = ref_norm['ener_low_prom']['mu'], ref_norm['ener_low_prom']['sd']
            z_low = (ener_low_prom - muL)/(sdL if sdL>0 else 1e-6)
        else:
            z_low = 0.0

        S, sev = severidad_score(z_mae, z_amp, z_low, dur_s)

        # tiempos absolutos o relativos
        if (t_parsed is not None):
            t_ini = t_parsed.iloc[r0] if modo_t=='abs' else pd.NaT
            t_fin = t_parsed.iloc[r1] if modo_t=='abs' else pd.NaT
            t_ini_s = (r0)*dt_s if modo_t!='abs' else np.nan
            t_fin_s = (r1)*dt_s if modo_t!='abs' else np.nan
        else:
            t_ini, t_fin, t_ini_s, t_fin_s = pd.NaT, pd.NaT, (r0)*dt_s, (r1)*dt_s

        rows.append(dict(
            dataset=base, tipo_hint=tipo_hint,
            t_ini=t_ini, t_fin=t_fin, t_ini_s=t_ini_s, t_fin_s=t_fin_s,
            duracion_s=dur_s,
            MAE_pico=mae_pico, MAE_prom=mae_prom, amplitud_pico=amp_pico, z_ener_low=z_low,
            severidad=sev
        ))

df_evt = pd.DataFrame(rows)
if df_evt.empty:
    raise RuntimeError("No se generaron eventos. Baja un poco 'threshold' o prueba N_CONSEC=2 temporalmente.")

# ---------- Métricas por dataset (eventos y duraciones) ----------
print("\n=== EVENTOS por dataset ===")
evt_counts = df_evt.groupby('tipo_hint').size().sort_values(ascending=False)
print(evt_counts.to_string())

print("\n=== DURACIONES (s) por dataset (media/mediana/p95) ===")
dur_stats = df_evt.groupby('tipo_hint')['duracion_s'].agg(['mean','median',lambda s: np.percentile(s,95)])
dur_stats = dur_stats.rename(columns={'<lambda_0>':'p95'})
print(dur_stats.round(2).to_string())

# ---------- CLASIFICACIÓN de tipo (si hay suficiente variedad) ----------
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score

feature_cols = ['MAE_pico','MAE_prom','duracion_s','amplitud_pico']  # básicas y robustas
tipos, counts = np.unique(df_evt['tipo_hint'].values, return_counts=True)
tipos_validos = tipos[counts >= 2]   # al menos 2 eventos por clase para poder evaluar
use_rf = (len(tipos_validos) >= 2)

if use_rf:
    mask = df_evt['tipo_hint'].isin(tipos_validos).values
    X = df_evt.loc[mask, feature_cols].fillna(0.0).values.astype(np.float32)
    y = df_evt.loc[mask, 'tipo_hint'].values
    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=RANDOM_SEED)
    tr_idx, te_idx = next(sss.split(X, y))
    Xtr, Xte, ytr, yte = X[tr_idx], X[te_idx], y[tr_idx], y[te_idx]

    from sklearn.ensemble import RandomForestClassifier
    rf = RandomForestClassifier(
        n_estimators=300, class_weight="balanced",
        random_state=RANDOM_SEED, n_jobs=-1
    )
    rf.fit(Xtr, ytr)
    yhat = rf.predict(Xte)

    print("\n=== CLASIFICADOR (RF) ===")
    print(classification_report(yte, yhat, digits=3))
    print("Matriz de confusión:\n", confusion_matrix(yte, yhat))
    print("Accuracy:", round(accuracy_score(yte, yhat),3), " Macro-F1:", round(f1_score(yte, yhat, average='macro'),3))

    # Predigo TODO el conjunto para la tabla
    yhat_all = rf.predict(df_evt[feature_cols].fillna(0.0).values.astype(np.float32))
    df_evt['tipo'] = yhat_all
    df_evt['origen_tipo'] = "modelo"
else:
    print("\n=== CLASIFICADOR (RF) ===")
    print("No hay suficientes eventos por clase (≥2) para evaluar entrenando/pronosticando de forma robusta.")
    df_evt['tipo'] = df_evt['tipo_hint']      # fallback
    df_evt['origen_tipo'] = "heuristica"

# ---------- ABCD para TODOS los eventos ----------
df_evt['ABCD'] = [
    decidir_ABCD(tp, int(sev), float(amp), ref_amp_p99, ref_p99_9, float(zlow))
    for tp, sev, amp, zlow in zip(df_evt['tipo'], df_evt['severidad'], df_evt['amplitud_pico'], df_evt['z_ener_low'])
]

print("\n=== Distribución ABCD (todas las detecciones) ===")
print(df_evt['ABCD'].value_counts().to_string())

# ---------- TABLA FINAL pedida ----------
has_abs_time = df_evt['t_ini'].notna().any()
if has_abs_time:
    tabla = df_evt[['tipo','t_ini','t_fin','duracion_s','ABCD','origen_tipo']].copy()
    try:
        tabla['t_ini'] = pd.to_datetime(tabla['t_ini']).dt.tz_localize(None)
        tabla['t_fin'] = pd.to_datetime(tabla['t_fin']).dt.tz_localize(None)
    except Exception:
        pass
else:
    tabla = df_evt[['tipo','t_ini_s','t_fin_s','duracion_s','ABCD','origen_tipo']].copy()
    tabla.rename(columns={'t_ini_s':'t_ini_seg','t_fin_s':'t_fin_seg'}, inplace=True)

tabla['duracion_s'] = tabla['duracion_s'].round(2)

print("\n=== TABLA DE EVENTOS (tipo, tiempo, ABCD) ===")
print(tabla.to_string(index=False))

# Guardar CSV
out_name = 'tabla_eventos_ABCD.csv'
tabla.to_csv(out_name, index=False)
print(f"\nArchivo guardado: {out_name}")

import pickle
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler

with open("scaler.pk", "wb") as f:
    pickle.dump(scaler, f)

print("Scaler guardado como 'scaler.pk'")

with open("Modelo_v01_01.pk", "wb") as f:
    pickle.dump(model, f)

print("Modelo entrenado y guardado como 'Modelo_v01_01.pk'")

"""# **Ejecutador de Modelo**
La idea es preparar los datos para el modelo ( ya entrenado) y este entrega resultados

**Importar el escalador de datos y modelo**
"""

import pickle
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler

with open("scaler.pk", "rb") as f:
    scaler = pickle.load(f)

print("Scaler cargado desde 'scaler.pk'")


X_nuevos = df_nuevos.select_dtypes(include=[np.number]).values

with open("Modelo_v01_01.pk", "rb") as f:
    autoencoder = pickle.load(f)

print("Modelo cargado desde 'Modelo_v01_01.pk'")


scaler = StandardScaler()
X_nuevos_scaled = scaler.fit_transform(X_nuevos)


resultados = autoencoder.predict(X_nuevos_scaled)

print("Resultados generados por el modelo:")
print(resultados[:5])