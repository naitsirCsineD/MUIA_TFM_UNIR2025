# -*- coding: utf-8 -*-
"""Modelo01_v1_01.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wGOoZWcYAkRYm-Vu8YlOtKE_IC3THhPs
"""

# ==========================
# REPRODUCIBILIDAD Y UMBRAL
# ==========================
# (1) Determinismo fuerte
import os, random
os.environ["PYTHONHASHSEED"] = "0"
os.environ["TF_DETERMINISTIC_OPS"] = "1"
os.environ["TF_CUDNN_DETERMINISTIC"] = "1"

import numpy as np
random.seed(0)
np.random.seed(0)

#  importa TensorFlow y configura hilos
import tensorflow as tf
tf.random.set_seed(0)
tf.config.threading.set_inter_op_parallelism_threads(1)
tf.config.threading.set_intra_op_parallelism_threads(1)

# ==========================
# LIBRER√çAS (set original)
# ==========================
import pandas as pd
import matplotlib.pyplot as plt
import io
import seaborn as sns
from google.colab import files

from sklearn.preprocessing import RobustScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, RepeatVector, TimeDistributed
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score

print("Semillas fijadas y ops deterministas activadas.")

# ==========================
# Bloque 0: subido de archivo
# ==========================
print("Sube el archivo NORMAL (ej: 'WindTurbine_...csv')")
uploaded_normal = files.upload()
file_normal = list(uploaded_normal.keys())[0]
print(f"Listo: {file_normal}")

print("\nSube el archivo con eventos (ej: 'Train_...csv')")
uploaded_anom = files.upload()
file_anom = list(uploaded_anom.keys())[0]
print(f"Listo: {file_anom}")

# Par√°metros
TIME_STEPS = 100  # tama√±o de la ventana para el LSTM

# --- Funci√≥n para armar secuencias 3D (samples, time_steps, features) ---
def crear_secuencias(data_2d, time_steps):
    X = []
    for i in range(len(data_2d) - time_steps):
        X.append(data_2d[i:(i + time_steps)])
    return np.array(X)

# ==========================
# 1) Leo CSV normal, ordeno, columnas
# ==========================
df_norm = pd.read_csv(io.BytesIO(uploaded_normal[file_normal]), low_memory=False)
df_norm = df_norm.sort_values("timestamp").reset_index(drop=True)

cols_features = ["amplitud_max"] + [f"dB_{i}Hz" for i in range(1, 21)]
cols_features = [c for c in cols_features if c in df_norm.columns]

X_norm_all = df_norm[cols_features].copy()
print("filas normales totales:", len(X_norm_all))
print("columnas usadas:", cols_features)

# ==========================
# 2) Split temporal: 70%/30%
# ==========================
n = len(X_norm_all)
i_train = int(n * 0.70)
X_train_df = X_norm_all.iloc[:i_train].copy()
X_val_df   = X_norm_all.iloc[i_train:].copy()

# quitar columnas constantes (IQR=0) en TRAIN
iqr = (X_train_df.quantile(0.75) - X_train_df.quantile(0.25))
const_cols = list(iqr[iqr == 0].index)
if const_cols:
    print("Columnas constantes en TRAIN (las quito):", const_cols)
    X_train_df = X_train_df.drop(columns=const_cols)
    X_val_df   = X_val_df.drop(columns=const_cols)
    cols_features = [c for c in cols_features if c not in const_cols]

# ==========================
# 3) Escalado robusto
# ==========================
scaler = RobustScaler()
X_train_s = scaler.fit_transform(X_train_df)
X_val_s   = scaler.transform(X_val_df)

# ==========================
# 4) Secuencias
# ==========================
X_train = crear_secuencias(X_train_s, TIME_STEPS)
X_val   = crear_secuencias(X_val_s, TIME_STEPS)
y_train = X_train
y_val   = X_val

print("shape X_train:", X_train.shape)
print("shape X_val:  ", X_val.shape)

# ==========================
# 5) Autoencoder LSTM (tanh)
#    + Limpio sesi√≥n para evitar estado previo
# ==========================
tf.keras.backend.clear_session()
n_features = X_train.shape[2]

model = Sequential()
# encoder
model.add(LSTM(64, activation='tanh', return_sequences=True, input_shape=(TIME_STEPS, n_features)))
model.add(LSTM(16, activation='tanh', return_sequences=False))
# decoder
model.add(RepeatVector(TIME_STEPS))
model.add(LSTM(16, activation='tanh', return_sequences=True))
model.add(LSTM(64, activation='tanh', return_sequences=True))
model.add(TimeDistributed(Dense(n_features)))

model.compile(optimizer='adam', loss='mae')
model.summary()

es  = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
rlr = ReduceLROnPlateau(monitor='val_loss', patience=5, factor=0.5, verbose=1)

hist = model.fit(
    X_train, y_train,
    epochs=100,
    batch_size=64,
    validation_data=(X_val, y_val),
    shuffle=False,
    callbacks=[es, rlr],
    verbose=1
)

# ==========================
# Curva de p√©rdida
# ==========================
plt.figure()
plt.plot(hist.history['loss'], label='entrenamiento')
plt.plot(hist.history['val_loss'], label='validaci√≥n')
plt.legend()
plt.title('p√©rdida durante el entrenamiento')
plt.xlabel('√©poca')
plt.ylabel('MAE')
plt.tight_layout()
plt.savefig('training_loss.png')
plt.close()
print("gr√°fico 'training_loss.png' guardado.")

# ==========================
# 6) Umbral (estable con fallback a archivo)
# ==========================
# Intento cargar un umbral fijo de una corrida previa, si existe:
threshold_file = "threshold.txt"
threshold = None
if os.path.exists(threshold_file):
    try:
        with open(threshold_file, "r") as f:
            threshold = float(f.read().strip())
        print(f"Umbral cargado desde '{threshold_file}': {threshold}")
    except:
        threshold = None

# Si no existe, lo calculo (p99.5) y LO GUARDO para las pr√≥ximas corridas
X_val_pred = model.predict(X_val, verbose=0)
val_mae = np.mean(np.abs(X_val_pred - X_val), axis=(1, 2))

if threshold is None:
    threshold = np.quantile(val_mae, 0.995)
    with open(threshold_file, "w") as f:
        f.write(str(threshold))
    print("Umbral calculado (p99.5 en validaci√≥n) y guardado en 'threshold.txt':", threshold)
else:
    print("Usando umbral fijo previamente guardado.")

plt.figure()
plt.hist(val_mae, bins=50)
plt.axvline(threshold, linestyle='--')
plt.title('distribuci√≥n del error (validaci√≥n)')
plt.xlabel('MAE por secuencia')
plt.ylabel('frecuencia')
plt.tight_layout()
plt.savefig('error_val_hist.png')
plt.close()
print("gr√°fico 'error_val_hist.png' guardado.")

# ==========================
# 7) Proceso para anomal√≠as
# ==========================
df_anom = pd.read_csv(io.BytesIO(uploaded_anom[file_anom]), low_memory=False)
df_anom = df_anom.sort_values("timestamp").reset_index(drop=True)

cols_comunes = [c for c in cols_features if c in df_anom.columns]
if set(cols_features) - set(cols_comunes):
    print("alerta: faltan columnas en el archivo de anomal√≠as:", list(set(cols_features) - set(cols_comunes)))

X_anom_df = df_anom[cols_comunes].copy()
X_anom_s = scaler.transform(X_anom_df)

X_anom = crear_secuencias(X_anom_s, TIME_STEPS)
X_anom_pred = model.predict(X_anom, verbose=0)
anom_mae = np.mean(np.abs(X_anom_pred - X_anom), axis=(1, 2))

flags = anom_mae > threshold
print(f"secuencias an√≥malas detectadas: {np.sum(flags)} de {len(flags)}")

plt.figure()
plt.hist(anom_mae, bins=50)
plt.axvline(threshold, linestyle='--')
plt.title('errores en archivo de anomal√≠as (comparado con el umbral)')
plt.xlabel('MAE por secuencia')
plt.ylabel('frecuencia')
plt.tight_layout()
plt.savefig('error_anom_hist.png')
plt.close()
print("gr√°fico 'error_anom_hist.png' guardado.")

#  M√©tricas de eficacia (evaluo con VALIDACI√ìN vs ANOMAL√çAS)


# En este punto ya tengo:
# - val_mae: errores MAE de validaci√≥n (normal)
# - anom_mae: errores MAE del archivo de anomal√≠as
# - threshold: umbral elegido (p99.5 de val_mae)

# 1) Armo las etiquetas verdaderas (0 = normal, 1 = an√≥malo)
y_true = np.concatenate([
    np.zeros(len(val_mae), dtype=int),   # todo esto es normal
    np.ones(len(anom_mae), dtype=int)    # esto es an√≥malo
])

# 2) Convierto los errores en predicciones binarias usando el umbral
#    si el error pasa el umbral => an√≥malo (1), si no => normal (0)
pred_norm  = (val_mae  > threshold).astype(int)
pred_anom  = (anom_mae > threshold).astype(int)
y_pred = np.concatenate([pred_norm, pred_anom])

# 3) Tambi√©n preparo un vector de "scores" continuo para AUROC/AUPRC
#    uso el MAE tal cual como "score de anomal√≠a": mayor MAE = m√°s an√≥malo
y_score = np.concatenate([val_mae, anom_mae])

# 4) Matriz de confusi√≥n
cm = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Pred. Normal', 'Pred. An√≥malo'],
            yticklabels=['Real Normal', 'Real An√≥malo'])
plt.title('Matriz de confusi√≥n (val vs anom)')
plt.xlabel('Predicci√≥n')
plt.ylabel('Real')
plt.tight_layout()
plt.savefig('confusion_matrix.png')
plt.show()
print("Gr√°fico 'confusion_matrix.png' guardado.")

# 5) M√©tricas con el umbral seleccionado
precision = precision_score(y_true, y_pred, zero_division=0)
recall    = recall_score(y_true, y_pred, zero_division=0)
f1        = f1_score(y_true, y_pred, zero_division=0)

print("\n--- M√©tricas con el umbral elegido ---")
print(f"Precisi√≥n: {precision:.4f}")
print(f"Sensibilidad (Recall): {recall:.4f}")
print(f"F1-Score: {f1:.4f}")

# 6) M√©tricas globales sin umbral
#    AUROC y AUPRC usan el score continuo (MAE)
try:
    auroc = roc_auc_score(y_true, y_score)
    auprc = average_precision_score(y_true, y_score)
    print("\n--- M√©tricas sin umbral ---")
    print(f"AUROC (ROC AUC): {auroc:.4f}")
    print(f"AUPRC (PR AUC): {auprc:.4f}")
except Exception as e:
    print("\nNo pude calcular AUROC/AUPRC:", e)

# 7)  Resumen de ‚Äúfalsos positivos sobre validaci√≥n‚Äù
fp = cm[0,1]  # normales que marc√≥ como an√≥malas
tn = cm[0,0]
fn = cm[1,0]
tp = cm[1,1]
print(f"\nResumen r√°pido:")
print(f"  Falsos positivos en validaci√≥n (normales marcados como an√≥malos): {fp}")
print(f"  Falsos negativos en anomal√≠as (an√≥malos marcados como normales): {fn}")

"""---

# Modelo **clasificador**

C Basic 28 agosto 2025

# **Guardar Tranformaciones de datos y Modelo Entrenado**
La idea es guardarlos y compartirlos en otros software
"""

# =========================================================
# TABLA + M√âTRICAS: Subir CSV ‚Üí Eventos ‚Üí Severidad/ABCD ‚Üí Clasificaci√≥n
# =========================================================
import os, numpy as np, pandas as pd, tensorflow as tf

# ---------- Requisitos del AE (ya corrido) ----------
_required = ['model','scaler','cols_features','TIME_STEPS','val_mae','df_norm','threshold','crear_secuencias']
_missing  = [v for v in _required if v not in globals()]
if _missing:
    raise RuntimeError(f"Faltan variables del AE: {_missing}. Ejecuta primero tu bloque del autoencoder.")

# ---------- Par√°metros ----------
N_CONSEC = 3     # ventanas consecutivas para formar evento
GAP_S    = 10.0  # unir bloques con huecos < GAP_S
RANDOM_SEED = 0

# ---------- Utilidades ----------
def _parse_timestamp_series(s: pd.Series):
    """
    Devuelve (t_datetime, dt_s, modo):
      - t_datetime: serie datetime si es plausible (>= 2000-01-01), si no None
      - dt_s: paso temporal medio en segundos (robusto)
      - modo: 'abs' (absoluto), 'rel' (relativo), 'none' (no se pudo)
    """
    try:
        t = pd.to_datetime(s, errors='coerce')
        if not t.isna().all():
            diffs = t.astype('int64').diff().dropna().values.astype(np.float64)
            dt_s = float(np.median(diffs)/1e9) if len(diffs)>0 else 1.0
            if t.dropna().iloc[0] >= pd.Timestamp('2000-01-01'):
                return (t, dt_s if dt_s>0 else 1.0, 'abs')
    except Exception:
        pass
    try:
        x = pd.to_numeric(s, errors='coerce')
        if x.isna().all(): return (None, 1.0, 'none')
        dif = np.diff(x.dropna().values)
        dt_s = float(np.median(dif)) if len(dif)>0 else 1.0
        # heur√≠stica de unidades largas (ms, ns): normalizo a segundos
        xmax = float(np.nanmax(x.values))
        if xmax > 1e12:   dt_s /= 1e9  # ns ‚Üí s
        elif xmax > 1e10: dt_s /= 1e3  # ms ‚Üí s
        return (None, dt_s if dt_s>0 else 1.0, 'rel')
    except Exception:
        return (None, 1.0, 'none')

def _band_cols(cols, lo=(1,5), mid=(5,15)):
    lo_cols = [f"dB_{i}Hz" for i in range(lo[0], lo[1]+1) if f"dB_{i}Hz" in cols]
    mid_cols= [f"dB_{i}Hz" for i in range(mid[0], mid[1]+1) if f"dB_{i}Hz" in cols]
    return lo_cols, mid_cols

def mae_por_secuencia(ae: tf.keras.Model, X_seq: np.ndarray) -> np.ndarray:
    X_pred = ae.predict(X_seq, verbose=0)
    return np.mean(np.abs(X_pred - X_seq), axis=(1, 2))

def flags_anomalia(mae_seq: np.ndarray, umbral: float, n_consec: int=3) -> np.ndarray:
    base = (mae_seq > umbral).astype(int)
    if n_consec<=1: return base.astype(bool)
    runs = np.convolve(base, np.ones(n_consec, dtype=int), mode='same')
    return (runs >= n_consec)

def agrupar_eventos(flags: np.ndarray, dt_s: float, gap_s: float=10.0):
    idx = np.where(flags)[0]
    if len(idx)==0: return []
    evs, s, p = [], idx[0], idx[0]
    for i in idx[1:]:
        gap = i - p - 1
        if gap>0 and gap*dt_s >= gap_s:
            evs.append((s, p)); s = i
        p = i
    evs.append((s, p))
    return evs

def _seq_to_rows(seq_i: int, seq_j: int, time_steps: int):
    return seq_i, seq_j + time_steps - 1

def stats_normalidad(df_normal: pd.DataFrame):
    ref={}
    if 'amplitud_max' in df_normal.columns:
        mu, sd = float(df_normal['amplitud_max'].mean()), float(df_normal['amplitud_max'].std() or 1e-6)
        ref['amplitud_max']={'mu':mu,'sd':sd if sd>0 else 1e-6}
    lo_cols,_ = _band_cols(df_normal.columns)
    if len(lo_cols)>0:
        v = df_normal[lo_cols].mean(axis=1)
        mu, sd = float(v.mean()), float(v.std() or 1e-6)
        ref['ener_low_prom']={'mu':mu,'sd':sd if sd>0 else 1e-6}
    return ref

def severidad_score(z_mae, z_amp, z_low, dur_s):
    # Pesos + cortes m√°s conservadores
    S = 0.30*z_mae + 0.25*z_amp + 0.20*z_low + 0.25*(dur_s/20.0)
    S = float(np.clip(S, -3, 8))
    if S < 1.5: sev = 1
    elif S < 3.0: sev = 2
    elif S < 4.5: sev = 3
    else: sev = 4
    return S, sev

def decidir_ABCD(tipo: str, severidad: int,
                 pico_amp: float,
                 ref_amp_p99: float = None,
                 ref_amp_p99_9: float = None,
                 z_low: float = 0.0) -> str:
    """
    D: solo si severidad=4 y (tipo=='sismo' o pico_amp‚â•p99.9 o z_low‚â•5)
    C: severidad=4 (no cr√≠tico) o severidad=3
    B: severidad=2
    A: severidad=1
    """
    t = (tipo or '').lower()
    critico = (t == 'sismo')
    limite_duro = ((ref_amp_p99_9 is not None and pico_amp >= ref_amp_p99_9) or (z_low >= 5.0))
    if severidad == 4 and (critico or limite_duro): return 'D'
    if severidad >= 3: return 'C'
    if severidad == 2: return 'B'
    return 'A'

# ---------- Subida de CSVs ----------
try:
    from google.colab import files
except Exception as e:
    raise RuntimeError("Este bloque de subida requiere Google Colab.") from e

print("üîº Selecciona UNO o VARIOS CSV (Sismo, Tren, Sirenas, Truenos, Helic√≥ptero, etc.)")
_uploaded = files.upload()
paths_csv = []
for name, data in _uploaded.items():
    with open(name, "wb") as f:
        f.write(data)
    paths_csv.append(name)
print("Archivos cargados:", [os.path.basename(p) for p in paths_csv])

# ---------- Referencias de normalidad ----------
ref_norm     = stats_normalidad(df_norm)
val_mae_mu   = float(np.mean(val_mae))
val_mae_sd   = float(np.std(val_mae) or 1e-6)
ref_amp_p99  = float(np.percentile(df_norm['amplitud_max'].values, 99.0))  if 'amplitud_max' in df_norm.columns else None
ref_p99_9    = float(np.percentile(df_norm['amplitud_max'].values, 99.9)) if 'amplitud_max' in df_norm.columns else None

# ---------- M√âTRICAS del detector sobre NORMALIDAD ----------
# Usamos val_mae (ventanas de normalidad) para estimar FPR por ventana y falsos eventos
fpr_windows = float((val_mae > threshold).mean()) if isinstance(val_mae, np.ndarray) and len(val_mae)>0 else np.nan
# Duraci√≥n promedio de una ventana ~ dt_s (estimado de df_norm)
_, dt_norm, _ = _parse_timestamp_series(df_norm['timestamp']) if 'timestamp' in df_norm.columns else (None, 1.0, 'none')
flags_norm   = flags_anomalia(val_mae, umbral=threshold, n_consec=N_CONSEC) if isinstance(val_mae, np.ndarray) else np.array([])
evs_norm     = agrupar_eventos(flags_norm, dt_norm, gap_s=GAP_S) if len(flags_norm)>0 else []
print("\n=== DETECTOR (sanidad sobre normalidad) ===")
print(f"FPR por ventana (normalidad): {fpr_windows:.5f}")
print(f"Falsos eventos en normalidad (N_CONSEC={N_CONSEC}, GAP={GAP_S}s): {len(evs_norm)}")

# ---------- Construcci√≥n de eventos y features (para todos los CSV) ----------
def _tipo_por_nombre(base: str) -> str:
    b = base.lower()
    if   'sismo' in b: return 'sismo'
    if   'train' in b or 'tren' in b: return 'tren'
    if   'sirena' in b: return 'sirena'
    if   'trueno' in b or 'thunder' in b: return 'trueno'
    if   'helicop' in b: return 'helicoptero'
    return 'otros'

rows = []
for path_csv in paths_csv:
    base = os.path.splitext(os.path.basename(path_csv))[0]
    tipo_hint = _tipo_por_nombre(base)
    df = pd.read_csv(path_csv).sort_values("timestamp").reset_index(drop=True)

    cols_comunes = [c for c in cols_features if c in df.columns]
    if len(cols_comunes)==0:
        print(f"[WARN] {os.path.basename(path_csv)} sin columnas comunes con {cols_features}; salto.")
        continue

    t_parsed, dt_s, modo_t = _parse_timestamp_series(df['timestamp']) if 'timestamp' in df.columns else (None, 1.0, 'none')

    Xs = scaler.transform(df[cols_comunes].copy())
    X_seq = crear_secuencias(Xs, TIME_STEPS)
    if len(X_seq)==0:
        print(f"[WARN] {os.path.basename(path_csv)}: no se formaron secuencias.")
        continue
    mae_seq = mae_por_secuencia(model, X_seq)

    flags = flags_anomalia(mae_seq, umbral=threshold, n_consec=N_CONSEC)
    evs   = agrupar_eventos(flags, dt_s, gap_s=GAP_S)

    lo_cols, _ = _band_cols(df.columns)
    for k,(s,e) in enumerate(evs,1):
        r0,r1 = _seq_to_rows(s,e,TIME_STEPS); r1 = min(r1,len(df)-1)
        nfil  = max(1, r1-r0+1)
        dur_s = nfil * dt_s
        v_mae = mae_seq[s:e+1]
        mae_pico = float(np.max(v_mae)); mae_prom = float(np.mean(v_mae))
        amp_pico = float(df['amplitud_max'].iloc[r0:r1+1].max()) if 'amplitud_max' in df.columns else np.nan
        ener_low_prom = float(df[lo_cols].iloc[r0:r1+1].mean().mean()) if len(lo_cols)>0 else np.nan

        # z-scores para severidad
        z_mae = (mae_pico - val_mae_mu)/val_mae_sd
        if 'amplitud_max' in ref_norm:
            muA, sdA = ref_norm['amplitud_max']['mu'], ref_norm['amplitud_max']['sd']
            z_amp = (amp_pico - muA)/(sdA if sdA>0 else 1e-6)
        else:
            z_amp = 0.0
        if 'ener_low_prom' in ref_norm and not np.isnan(ener_low_prom):
            muL, sdL = ref_norm['ener_low_prom']['mu'], ref_norm['ener_low_prom']['sd']
            z_low = (ener_low_prom - muL)/(sdL if sdL>0 else 1e-6)
        else:
            z_low = 0.0

        S, sev = severidad_score(z_mae, z_amp, z_low, dur_s)

        # tiempos absolutos o relativos
        if (t_parsed is not None):
            t_ini = t_parsed.iloc[r0] if modo_t=='abs' else pd.NaT
            t_fin = t_parsed.iloc[r1] if modo_t=='abs' else pd.NaT
            t_ini_s = (r0)*dt_s if modo_t!='abs' else np.nan
            t_fin_s = (r1)*dt_s if modo_t!='abs' else np.nan
        else:
            t_ini, t_fin, t_ini_s, t_fin_s = pd.NaT, pd.NaT, (r0)*dt_s, (r1)*dt_s

        rows.append(dict(
            dataset=base, tipo_hint=tipo_hint,
            t_ini=t_ini, t_fin=t_fin, t_ini_s=t_ini_s, t_fin_s=t_fin_s,
            duracion_s=dur_s,
            MAE_pico=mae_pico, MAE_prom=mae_prom, amplitud_pico=amp_pico, z_ener_low=z_low,
            severidad=sev
        ))

df_evt = pd.DataFrame(rows)
if df_evt.empty:
    raise RuntimeError("No se generaron eventos. Baja un poco 'threshold' o prueba N_CONSEC=2 temporalmente.")

# ---------- M√©tricas por dataset (eventos y duraciones) ----------
print("\n=== EVENTOS por dataset ===")
evt_counts = df_evt.groupby('tipo_hint').size().sort_values(ascending=False)
print(evt_counts.to_string())

print("\n=== DURACIONES (s) por dataset (media/mediana/p95) ===")
dur_stats = df_evt.groupby('tipo_hint')['duracion_s'].agg(['mean','median',lambda s: np.percentile(s,95)])
dur_stats = dur_stats.rename(columns={'<lambda_0>':'p95'})
print(dur_stats.round(2).to_string())

# ---------- CLASIFICACI√ìN de tipo (si hay suficiente variedad) ----------
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score

feature_cols = ['MAE_pico','MAE_prom','duracion_s','amplitud_pico']  # b√°sicas y robustas
tipos, counts = np.unique(df_evt['tipo_hint'].values, return_counts=True)
tipos_validos = tipos[counts >= 2]   # al menos 2 eventos por clase para poder evaluar
use_rf = (len(tipos_validos) >= 2)

if use_rf:
    mask = df_evt['tipo_hint'].isin(tipos_validos).values
    X = df_evt.loc[mask, feature_cols].fillna(0.0).values.astype(np.float32)
    y = df_evt.loc[mask, 'tipo_hint'].values
    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=RANDOM_SEED)
    tr_idx, te_idx = next(sss.split(X, y))
    Xtr, Xte, ytr, yte = X[tr_idx], X[te_idx], y[tr_idx], y[te_idx]

    from sklearn.ensemble import RandomForestClassifier
    rf = RandomForestClassifier(
        n_estimators=300, class_weight="balanced",
        random_state=RANDOM_SEED, n_jobs=-1
    )
    rf.fit(Xtr, ytr)
    yhat = rf.predict(Xte)

    print("\n=== CLASIFICADOR (RF) ===")
    print(classification_report(yte, yhat, digits=3))
    print("Matriz de confusi√≥n:\n", confusion_matrix(yte, yhat))
    print("Accuracy:", round(accuracy_score(yte, yhat),3), " Macro-F1:", round(f1_score(yte, yhat, average='macro'),3))

    # Predigo TODO el conjunto para la tabla
    yhat_all = rf.predict(df_evt[feature_cols].fillna(0.0).values.astype(np.float32))
    df_evt['tipo'] = yhat_all
    df_evt['origen_tipo'] = "modelo"
else:
    print("\n=== CLASIFICADOR (RF) ===")
    print("No hay suficientes eventos por clase (‚â•2) para evaluar entrenando/pronosticando de forma robusta.")
    df_evt['tipo'] = df_evt['tipo_hint']      # fallback
    df_evt['origen_tipo'] = "heuristica"

# ---------- ABCD para TODOS los eventos ----------
df_evt['ABCD'] = [
    decidir_ABCD(tp, int(sev), float(amp), ref_amp_p99, ref_p99_9, float(zlow))
    for tp, sev, amp, zlow in zip(df_evt['tipo'], df_evt['severidad'], df_evt['amplitud_pico'], df_evt['z_ener_low'])
]

print("\n=== Distribuci√≥n ABCD (todas las detecciones) ===")
print(df_evt['ABCD'].value_counts().to_string())

# ---------- TABLA FINAL pedida ----------
has_abs_time = df_evt['t_ini'].notna().any()
if has_abs_time:
    tabla = df_evt[['tipo','t_ini','t_fin','duracion_s','ABCD','origen_tipo']].copy()
    try:
        tabla['t_ini'] = pd.to_datetime(tabla['t_ini']).dt.tz_localize(None)
        tabla['t_fin'] = pd.to_datetime(tabla['t_fin']).dt.tz_localize(None)
    except Exception:
        pass
else:
    tabla = df_evt[['tipo','t_ini_s','t_fin_s','duracion_s','ABCD','origen_tipo']].copy()
    tabla.rename(columns={'t_ini_s':'t_ini_seg','t_fin_s':'t_fin_seg'}, inplace=True)

tabla['duracion_s'] = tabla['duracion_s'].round(2)

print("\n=== TABLA DE EVENTOS (tipo, tiempo, ABCD) ===")
print(tabla.to_string(index=False))

# Guardar CSV
out_name = 'tabla_eventos_ABCD.csv'
tabla.to_csv(out_name, index=False)
print(f"\nArchivo guardado: {out_name}")

import pickle
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler

with open("scaler.pk", "wb") as f:
    pickle.dump(scaler, f)

print("Scaler guardado como 'scaler.pk'")

with open("Modelo_v01_01.pk", "wb") as f:
    pickle.dump(model, f)

print("Modelo entrenado y guardado como 'Modelo_v01_01.pk'")

"""# **Ejecutador de Modelo**
La idea es preparar los datos para el modelo ( ya entrenado) y este entrega resultados

**Importar el escalador de datos y modelo**
"""

import pickle
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler

with open("scaler.pk", "rb") as f:
    scaler = pickle.load(f)

print("Scaler cargado desde 'scaler.pk'")


X_nuevos = df_nuevos.select_dtypes(include=[np.number]).values

with open("Modelo_v01_01.pk", "rb") as f:
    autoencoder = pickle.load(f)

print("Modelo cargado desde 'Modelo_v01_01.pk'")


scaler = StandardScaler()
X_nuevos_scaled = scaler.fit_transform(X_nuevos)


resultados = autoencoder.predict(X_nuevos_scaled)

print("Resultados generados por el modelo:")
print(resultados[:5])