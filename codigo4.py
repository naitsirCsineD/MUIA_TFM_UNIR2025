# -*- coding: utf-8 -*-
"""codigo4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V_9uShgNbJd2TvdWdGGsIcBAbycazcFI
"""

import os
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
from google.colab import drive
import warnings
warnings.filterwarnings('ignore')

# Montar Google Drive
drive.flush_and_unmount()
drive.mount('/content/drive', force_remount=True)

# ==================== CONFIGURACI√ìN ====================
BASE_PATH = "/content/drive/MyDrive/UNIR_Compartidos/TFM/DATA/SonSuper/csv/"

# ==================== EXTRACTOR DE CARACTER√çSTICAS ====================
class CSVFeatureExtractor:
    def __init__(self):
        self.required_columns = ['timestamp', 'amplitud_max'] + [f'dB_{i}Hz' for i in range(1, 21)]

    def extract_features(self, df):
        """Extraer caracter√≠sticas de DataFrame"""
        features = {}

        # Verificar y corregir nombres de columnas
        df = self._normalize_column_names(df)

        # Estad√≠sticas de amplitud
        if 'amplitud_max' in df.columns:
            features['amplitude_mean'] = df['amplitud_max'].mean()
            features['amplitude_std'] = df['amplitud_max'].std()
            features['amplitude_max'] = df['amplitud_max'].max()
            features['amplitude_min'] = df['amplitud_max'].min()
        else:
            print("‚ö†Ô∏è  Columna 'amplitud_max' no encontrada")
            return None

        # Caracter√≠sticas por frecuencia (1-20Hz)
        for i in range(1, 21):
            db_col = f'dB_{i}Hz'
            if db_col in df.columns:
                features[f'{db_col}_mean'] = df[db_col].mean()
                features[f'{db_col}_std'] = df[db_col].std()
                features[f'{db_col}_max'] = df[db_col].max()
                features[f'{db_col}_min'] = df[db_col].min()

        # Caracter√≠sticas espectrales
        spectral_means = []
        for i in range(1, 21):
            db_col = f'dB_{i}Hz'
            if db_col in df.columns:
                spectral_means.append(df[db_col].mean())

        if spectral_means:
            features['spectral_centroid'] = np.mean(spectral_means)
            features['spectral_bandwidth'] = np.std(spectral_means)
            features['spectral_rolloff'] = np.percentile(spectral_means, 85)

        return np.array(list(features.values()))

    def _normalize_column_names(self, df):
        """Normalizar nombres de columnas"""
        column_mapping = {}
        for col in df.columns:
            col_lower = col.strip().lower()

            # Mapear a nombres estandarizados
            if 'amplitud' in col_lower or 'amplitude' in col_lower:
                column_mapping[col] = 'amplitud_max'
            elif 'timestamp' in col_lower or 'time' in col_lower:
                column_mapping[col] = 'timestamp'
            elif 'db' in col_lower or 'decibel' in col_lower:
                # Extraer n√∫mero de frecuencia
                import re
                match = re.search(r'(\d+)', col)
                if match:
                    freq = match.group(1)
                    column_mapping[col] = f'dB_{freq}Hz'

        return df.rename(columns=column_mapping) if column_mapping else df

# ==================== RED NEURONAL ====================
class EnhancedDangerNeuralNetwork(nn.Module):
    def __init__(self, input_dim, num_classes):
        super(EnhancedDangerNeuralNetwork, self).__init__()

        self.network = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.ReLU(),
            nn.Dropout(0.4),

            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(0.3),

            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Dropout(0.2),

            nn.Linear(32, num_classes)
        )

        self.risk_assessor = nn.Sequential(
            nn.Linear(num_classes, 16),
            nn.ReLU(),
            nn.Linear(16, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        class_logits = self.network(x)
        risk_score = self.risk_assessor(class_logits)
        return class_logits, risk_score

# ==================== DATASET ====================
class StructuralSoundDataset(Dataset):
    def __init__(self, features, labels, risk_scores):
        self.features = features
        self.labels = labels
        self.risk_scores = risk_scores

    def __len__(self):
        return len(self.features)

    def __getitem__(self, idx):
        return (
            torch.FloatTensor(self.features[idx]),
            torch.LongTensor([self.labels[idx]]).squeeze(),  # Cambiado: squeeze para quedar como escalar
            torch.FloatTensor([self.risk_scores[idx]]).squeeze()  # Cambiado: squeeze para quedar como escalar
        )

# ==================== SISTEMA PRINCIPAL ====================
class EnhancedStructuralSafetySystem:
    def __init__(self):
        self.csv_extractor = CSVFeatureExtractor()
        self.scaler = StandardScaler()
        self.label_encoder = LabelEncoder()
        self.model = None
        self.is_trained = False
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.history = {'train_loss': [], 'val_accuracy': [], 'val_f1': [], 'risk_error': []}
        print(f"Usando dispositivo: {self.device}")

    def load_csv_data(self, csv_path):
        """Cargar datos desde archivo CSV"""
        try:
            df = pd.read_csv(csv_path)
            # Verifica que tenga las columnas necesarias
            required_cols = ['timestamp', 'amplitud_max'] + [f'dB_{i}Hz' for i in range(1, 21)]
            for col in required_cols:
                if col not in df.columns:
                    print(f"‚ö†Ô∏è  Columna '{col}' no encontrada en {os.path.basename(csv_path)}")
                    return None
            return df
        except Exception as e:
            print(f"Error cargando {csv_path}: {e}")
            return None

    def prepare_training_data(self, csv_files, labels, risk_scores):
        """Preparar datos de entrenamiento desde CSVs"""
        print("Extrayendo caracter√≠sticas de archivos CSV...")

        features = []
        valid_labels = []
        valid_risk_scores = []
        failed_files = []

        for i, (csv_path, label, risk) in enumerate(zip(csv_files, labels, risk_scores)):
            try:
                # Cargar datos CSV
                csv_data = self.load_csv_data(csv_path)
                if csv_data is None:
                    failed_files.append(csv_path)
                    continue

                # Extraer caracter√≠sticas
                feature_vector = self.csv_extractor.extract_features(csv_data)
                if feature_vector is not None:
                    features.append(feature_vector)
                    valid_labels.append(label)
                    valid_risk_scores.append(risk)

                if (i + 1) % 5 == 0:
                    print(f"Procesados {i + 1}/{len(csv_files)} archivos")

            except Exception as e:
                print(f"Error procesando {csv_path}: {e}")
                failed_files.append(csv_path)

        if failed_files:
            print(f"\n‚ö†Ô∏è  Archivos con errores: {len(failed_files)}")

        if not features:
            raise ValueError("‚ùå No se pudieron extraer caracter√≠sticas de ning√∫n archivo")

        features = np.array(features)
        labels = np.array(valid_labels)
        risk_scores = np.array(valid_risk_scores)

        print(f"\n‚úÖ Caracter√≠sticas extra√≠das: {features.shape}")

        # Verificar distribuci√≥n de clases
        print("üìä Distribuci√≥n de clases:")
        class_distribution = Counter(labels)
        for class_name, count in class_distribution.items():
            print(f"   {class_name}: {count} muestras")

        # Normalizar caracter√≠sticas
        features = self.scaler.fit_transform(features)

        return features, labels, risk_scores

    def train_model(self, csv_files, labels, risk_scores, epochs=50, batch_size=4):
        """Entrenar el modelo con manejo de clases desbalanceadas - SOLUCI√ìN 1"""
        print("Iniciando entrenamiento...")

        # Preparar datos
        features, labels, risk_scores = self.prepare_training_data(csv_files, labels, risk_scores)

        # Verificar distribuci√≥n de clases
        unique_labels, counts = np.unique(labels, return_counts=True)
        num_classes = len(unique_labels)

        print(f"üéØ N√∫mero de clases: {num_classes}")
        print(f"üìä Distribuci√≥n de clases:")
        for label, count in zip(unique_labels, counts):
            print(f"   Clase {label}: {count} muestras")

        # SOLUCI√ìN 1: Usar todos los datos para entrenamiento (sin validaci√≥n)
        print("‚ö†Ô∏è  Pocos datos, usando TODOS para entrenamiento (sin validaci√≥n)")
        X_train, y_train, risk_train = features, labels, risk_scores

        print(f"üìä Datos de entrenamiento: {len(X_train)}")

        # Convierte a tensores directamente sin DataLoader para evitar problemas de dimensiones
        X_train_tensor = torch.FloatTensor(X_train).to(self.device)
        y_train_tensor = torch.LongTensor(y_train).to(self.device)
        risk_train_tensor = torch.FloatTensor(risk_train).to(self.device)

        # Inicializar modelo
        input_dim = X_train.shape[1]

        print(f"üìà Dimensiones de entrada: {input_dim}")
        print(f"üéØ N√∫mero de clases: {num_classes}")

        self.model = EnhancedDangerNeuralNetwork(input_dim, num_classes).to(self.device)

        # Definir loss functions y optimizer
        criterion_class = nn.CrossEntropyLoss()
        criterion_risk = nn.MSELoss()
        optimizer = optim.Adam(self.model.parameters(), lr=0.001, weight_decay=1e-5)

        # Entrenamiento sin DataLoader (usando todos los datos en cada √©poca)
        print(f"\nüéØ Comenzando entrenamiento por {epochs} √©pocas...")

        for epoch in range(epochs):
            # Fase de entrenamiento
            self.model.train()
            optimizer.zero_grad()

            # Forward pass con todos los datos
            class_logits, risk_pred = self.model(X_train_tensor)

            # Calcular p√©rdidas
            loss_class = criterion_class(class_logits, y_train_tensor)
            loss_risk = criterion_risk(risk_pred.squeeze(), risk_train_tensor)
            loss = loss_class + 0.3 * loss_risk

            # Backward pass
            loss.backward()
            optimizer.step()

            # Guardar p√©rdida
            self.history['train_loss'].append(loss.item())

            # Mostrar progreso cada 10 √©pocas
            if (epoch + 1) % 10 == 0:
                print(f'Epoch {epoch + 1}/{epochs}: Loss: {loss.item():.4f}')

        self.is_trained = True
        print("‚úÖ Entrenamiento completado!")

        # Evaluaci√≥n final solo con datos de entrenamiento
        self.print_comprehensive_metrics(X_train, y_train)

        # Graficar resultados
        self.plot_training_results()

    def print_comprehensive_metrics(self, X_data, y_data):
        """Imprimir m√©tricas completas del modelo"""
        print("\n" + "="*60)
        print("üìä M√âTRICAS COMPLETAS DEL MODELO")
        print("="*60)

        # Predicciones
        self.model.eval()
        with torch.no_grad():
            X_tensor = torch.FloatTensor(X_data).to(self.device)
            train_logits, _ = self.model(X_tensor)
            train_preds = torch.argmax(train_logits, dim=1).cpu().numpy()

        print(f"\nüìà Resultados en {len(X_data)} muestras:")
        print(f"Accuracy: {accuracy_score(y_data, train_preds):.4f}")
        print(f"Precision: {precision_score(y_data, train_preds, average='weighted', zero_division=0):.4f}")
        print(f"Recall: {recall_score(y_data, train_preds, average='weighted', zero_division=0):.4f}")
        print(f"F1-Score: {f1_score(y_data, train_preds, average='weighted', zero_division=0):.4f}")

        # Reporte de clasificaci√≥n
        print(f"\nValores reales √∫nicos: {np.unique(y_data)}")
        print(f"Predicciones √∫nicas: {np.unique(train_preds)}")

    def plot_training_results(self):
        """Graficar resultados del entrenamiento"""
        if not self.history['train_loss']:
            print("‚ö†Ô∏è  No hay datos de entrenamiento para graficar")
            return

        plt.figure(figsize=(8, 5))
        plt.plot(self.history['train_loss'], label='Training Loss', color='blue', linewidth=2)
        plt.title('P√©rdida de Entrenamiento')
        plt.xlabel('√âpoca')
        plt.ylabel('Loss')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.show()

    def predict_danger_level(self, csv_path):
        """Predecir nivel de peligro de un nuevo archivo CSV"""
        if not self.is_trained:
            raise ValueError("‚ùå El modelo debe ser entrenado primero")

        try:
            # Cargar y procesar datos
            csv_data = self.load_csv_data(csv_path)
            if csv_data is None:
                return {"error": "No se pudo cargar el archivo CSV"}

            features = self.csv_extractor.extract_features(csv_data)
            if features is None:
                return {"error": "No se pudieron extraer caracter√≠sticas"}

            # Normalizar y predecir
            features_scaled = self.scaler.transform(features.reshape(1, -1))
            features_tensor = torch.FloatTensor(features_scaled).to(self.device)

            self.model.eval()
            with torch.no_grad():
                class_logits, risk_score = self.model(features_tensor)
                probabilities = torch.softmax(class_logits, dim=1)
                predicted_class = torch.argmax(probabilities, dim=1).item()
                risk_value = risk_score.item()

            # Determinar nivel de peligro basado en el riesgo
            if risk_value > 0.7:
                danger_level = "ALTO PELIGRO"
                recommendation = "üö® EVACUACI√ìN INMEDIATA"
            elif risk_value > 0.5:
                danger_level = "PELIGRO MODERADO"
                recommendation = "‚ö†Ô∏è NO INGRESAR - REQUIERE INSPECCI√ìN"
            elif risk_value > 0.3:
                danger_level = "RIESGO BAJO"
                recommendation = "üî∂ PRECAUCI√ìN - MONITOREAR"
            else:
                danger_level = "SEGURO"
                recommendation = "‚úÖ ESTRUCTURA ESTABLE"

            return {
                'archivo': os.path.basename(csv_path),
                'puntuacion_riesgo': risk_value,
                'nivel_peligro': danger_level,
                'recomendacion': recommendation
            }

        except Exception as e:
            return {"error": f"Error en predicci√≥n: {str(e)}"}

# ==================== FUNCIONES PARA CARGAR DATOS ====================
def find_all_csv_files(base_path=BASE_PATH):
    """Encontrar todos los archivos CSV en el directorio"""
    csv_files = []
    for file in os.listdir(base_path):
        if file.endswith('.csv') and file != 'metadata.csv':
            csv_files.append(os.path.join(base_path, file))
    return csv_files

def automatic_labeling(filename):
    """Etiquetado autom√°tico basado en nombre de archivo"""
    filename_lower = filename.lower()

    # ETIQUETAS MEJORADAS PARA SONIDOS ESTRUCTURALES
    if 'sirena' in filename_lower:
        return 'alerta_sonora', 0.8
    elif 'arma' in filename_lower or 'disparo' in filename_lower:
        return 'peligro_alto', 0.9
    elif 'explosion' in filename_lower or 'bomba' in filename_lower:
        return 'peligro_alto', 0.95
    elif 'colapso' in filename_lower or 'derrumbe' in filename_lower:
        return 'peligro_alto', 0.9
    elif 'grieta' in filename_lower or 'agrietamiento' in filename_lower:
        return 'peligro_medio', 0.7
    elif 'romp' in filename_lower or 'quebrad' in filename_lower:
        return 'peligro_medio', 0.65
    elif 'metal' in filename_lower or 'acero' in filename_lower:
        return 'metal', 0.6
    elif 'madera' in filename_lower or 'wood' in filename_lower:
        return 'madera', 0.5
    elif 'concreto' in filename_lower or 'hormigon' in filename_lower:
        return 'concreto', 0.5
    elif 'terremoto' in filename_lower or 'sismo' in filename_lower:
        return 'natural', 0.8
    elif 'helicoptero' in filename_lower or 'avion' in filename_lower:
        return 'vehiculo', 0.6
    elif 'auto' in filename_lower or 'motor' in filename_lower:
        return 'vehiculo', 0.5
    elif 'maquina' in filename_lower or 'drill' in filename_lower:
        return 'maquinaria', 0.6
    elif 'viento' in filename_lower or 'tormenta' in filename_lower:
        return 'natural', 0.4
    else:
        return 'otros', 0.3

def load_training_data_automatic():
    """Cargar datos autom√°ticamente con etiquetado inteligente"""
    print("üìÇ Buscando archivos CSV autom√°ticamente...")

    # Buscar todos los archivos CSV
    csv_files = find_all_csv_files()

    if not csv_files:
        print("‚ùå No se encontraron archivos CSV")
        return [], [], []

    print(f"‚úÖ Encontrados {len(csv_files)} archivos CSV")

    labels = []
    risk_scores = []

    for csv_file in csv_files:
        filename = os.path.basename(csv_file)
        label, risk = automatic_labeling(filename)
        labels.append(label)
        risk_scores.append(risk)

        print(f"   {filename} -> {label} (riesgo: {risk})")

    # Mostrar distribuci√≥n
    print("\nüìä Distribuci√≥n autom√°tica de clases:")
    for label, count in Counter(labels).items():
        print(f"   {label}: {count} muestras")

    return csv_files, labels, risk_scores

# ==================== EJECUCI√ìN PRINCIPAL ====================
print("üöÄ INICIANDO ENTRENAMIENTO DEL MODELO...")
print("=" * 50)

# Cargar datos autom√°ticamente
csv_files, labels, risk_scores = load_training_data_automatic()

if not csv_files:
    print("‚ùå No hay datos para entrenar")
else:
    # Codificar etiquetas
    from sklearn.preprocessing import LabelEncoder
    label_encoder = LabelEncoder()
    labels_encoded = label_encoder.fit_transform(labels)

    print(f"üîß Codificaci√≥n aplicada:")
    for original, encoded in zip(labels, labels_encoded):
        print(f"   {original} -> {encoded}")

    print(f"\nüìä Distribuci√≥n de clases:")
    class_counts = Counter(labels_encoded)
    for label, count in class_counts.items():
        print(f"   Clase {label}: {count} muestras")

    print(f"üéØ N√∫mero total de clases: {len(label_encoder.classes_)}")

    # Verificar si hay clases con solo 1 muestra
    single_sample_classes = [cls for cls, count in class_counts.items() if count == 1]
    if single_sample_classes:
        print(f"‚ö†Ô∏è  Clases con solo 1 muestra: {single_sample_classes}")
        print("üìù Se usar√° validaci√≥n manual o todos los datos para entrenamiento")

    # Entrenar el modelo
    safety_system = EnhancedStructuralSafetySystem()
    safety_system.train_model(csv_files, labels_encoded, risk_scores, epochs=50, batch_size=4)

    # Probar predicci√≥n
    print("\nüîç PROBANDO PREDICCI√ìN...")
    if csv_files:
        sample_file = csv_files[0]
        prediction = safety_system.predict_danger_level(sample_file)
        print(f"\nüìã PREDICCI√ìN PARA: {os.path.basename(sample_file)}")
        print("=" * 40)
        for key, value in prediction.items():
            print(f"{key:.<20}: {value}")

        print("\nüéâ ¬°ENTRENAMIENTO COMPLETADO EXITOSAMENTE!")