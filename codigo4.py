# -*- coding: utf-8 -*-
"""codigo4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V_9uShgNbJd2TvdWdGGsIcBAbycazcFI
"""

import os
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
from google.colab import drive
import warnings
warnings.filterwarnings('ignore')

# Montar Google Drive
drive.flush_and_unmount()
drive.mount('/content/drive', force_remount=True)

# ==================== CONFIGURACIÃ“N ====================
BASE_PATH = "/content/drive/MyDrive/UNIR_Compartidos/TFM/DATA/SonSuper/csv/"

# ==================== EXTRACTOR DE CARACTERÃSTICAS ====================
class CSVFeatureExtractor:
    def __init__(self):
        self.required_columns = ['timestamp', 'amplitud_max'] + [f'dB_{i}Hz' for i in range(1, 21)]

    def extract_features(self, df):
        """Extraer caracterÃ­sticas de DataFrame"""
        features = {}

        # Verificar y corregir nombres de columnas
        df = self._normalize_column_names(df)

        # EstadÃ­sticas de amplitud
        if 'amplitud_max' in df.columns:
            features['amplitude_mean'] = df['amplitud_max'].mean()
            features['amplitude_std'] = df['amplitud_max'].std()
            features['amplitude_max'] = df['amplitud_max'].max()
            features['amplitude_min'] = df['amplitud_max'].min()
        else:
            print("âš ï¸  Columna 'amplitud_max' no encontrada")
            return None

        # CaracterÃ­sticas por frecuencia (1-20Hz)
        for i in range(1, 21):
            db_col = f'dB_{i}Hz'
            if db_col in df.columns:
                features[f'{db_col}_mean'] = df[db_col].mean()
                features[f'{db_col}_std'] = df[db_col].std()
                features[f'{db_col}_max'] = df[db_col].max()
                features[f'{db_col}_min'] = df[db_col].min()

        # CaracterÃ­sticas espectrales
        spectral_means = []
        for i in range(1, 21):
            db_col = f'dB_{i}Hz'
            if db_col in df.columns:
                spectral_means.append(df[db_col].mean())

        if spectral_means:
            features['spectral_centroid'] = np.mean(spectral_means)
            features['spectral_bandwidth'] = np.std(spectral_means)
            features['spectral_rolloff'] = np.percentile(spectral_means, 85)

        return np.array(list(features.values()))

    def _normalize_column_names(self, df):
        """Normalizar nombres de columnas"""
        column_mapping = {}
        for col in df.columns:
            col_lower = col.strip().lower()

            # Mapear a nombres estandarizados
            if 'amplitud' in col_lower or 'amplitude' in col_lower:
                column_mapping[col] = 'amplitud_max'
            elif 'timestamp' in col_lower or 'time' in col_lower:
                column_mapping[col] = 'timestamp'
            elif 'db' in col_lower or 'decibel' in col_lower:
                # Extraer nÃºmero de frecuencia
                import re
                match = re.search(r'(\d+)', col)
                if match:
                    freq = match.group(1)
                    column_mapping[col] = f'dB_{freq}Hz'

        return df.rename(columns=column_mapping) if column_mapping else df

# ==================== RED NEURONAL ====================
class EnhancedDangerNeuralNetwork(nn.Module):
    def __init__(self, input_dim, num_classes):
        super(EnhancedDangerNeuralNetwork, self).__init__()

        self.network = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.ReLU(),
            nn.Dropout(0.4),

            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(0.3),

            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Dropout(0.2),

            nn.Linear(32, num_classes)
        )

        self.risk_assessor = nn.Sequential(
            nn.Linear(num_classes, 16),
            nn.ReLU(),
            nn.Linear(16, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        class_logits = self.network(x)
        risk_score = self.risk_assessor(class_logits)
        return class_logits, risk_score

# ==================== DATASET ====================
class StructuralSoundDataset(Dataset):
    def __init__(self, features, labels, risk_scores):
        self.features = features
        self.labels = labels
        self.risk_scores = risk_scores

    def __len__(self):
        return len(self.features)

    def __getitem__(self, idx):
        return (
            torch.FloatTensor(self.features[idx]),
            torch.LongTensor([self.labels[idx]]).squeeze(),  # Cambiado: squeeze para quedar como escalar
            torch.FloatTensor([self.risk_scores[idx]]).squeeze()  # Cambiado: squeeze para quedar como escalar
        )

# ==================== SISTEMA PRINCIPAL ====================
class EnhancedStructuralSafetySystem:
    def __init__(self):
        self.csv_extractor = CSVFeatureExtractor()
        self.scaler = StandardScaler()
        self.label_encoder = LabelEncoder()
        self.model = None
        self.is_trained = False
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.history = {'train_loss': [], 'val_accuracy': [], 'val_f1': [], 'risk_error': []}
        print(f"Usando dispositivo: {self.device}")

    def load_csv_data(self, csv_path):
        """Cargar datos desde archivo CSV"""
        try:
            df = pd.read_csv(csv_path)
            # Verifica que tenga las columnas necesarias
            required_cols = ['timestamp', 'amplitud_max'] + [f'dB_{i}Hz' for i in range(1, 21)]
            for col in required_cols:
                if col not in df.columns:
                    print(f"âš ï¸  Columna '{col}' no encontrada en {os.path.basename(csv_path)}")
                    return None
            return df
        except Exception as e:
            print(f"Error cargando {csv_path}: {e}")
            return None

    def prepare_training_data(self, csv_files, labels, risk_scores):
        """Preparar datos de entrenamiento desde CSVs"""
        print("Extrayendo caracterÃ­sticas de archivos CSV...")

        features = []
        valid_labels = []
        valid_risk_scores = []
        failed_files = []

        for i, (csv_path, label, risk) in enumerate(zip(csv_files, labels, risk_scores)):
            try:
                # Cargar datos CSV
                csv_data = self.load_csv_data(csv_path)
                if csv_data is None:
                    failed_files.append(csv_path)
                    continue

                # Extraer caracterÃ­sticas
                feature_vector = self.csv_extractor.extract_features(csv_data)
                if feature_vector is not None:
                    features.append(feature_vector)
                    valid_labels.append(label)
                    valid_risk_scores.append(risk)

                if (i + 1) % 5 == 0:
                    print(f"Procesados {i + 1}/{len(csv_files)} archivos")

            except Exception as e:
                print(f"Error procesando {csv_path}: {e}")
                failed_files.append(csv_path)

        if failed_files:
            print(f"\nâš ï¸  Archivos con errores: {len(failed_files)}")

        if not features:
            raise ValueError("âŒ No se pudieron extraer caracterÃ­sticas de ningÃºn archivo")

        features = np.array(features)
        labels = np.array(valid_labels)
        risk_scores = np.array(valid_risk_scores)

        print(f"\nâœ… CaracterÃ­sticas extraÃ­das: {features.shape}")

        # Verificar distribuciÃ³n de clases
        print("ğŸ“Š DistribuciÃ³n de clases:")
        class_distribution = Counter(labels)
        for class_name, count in class_distribution.items():
            print(f"   {class_name}: {count} muestras")

        # Normalizar caracterÃ­sticas
        features = self.scaler.fit_transform(features)

        return features, labels, risk_scores

    def train_model(self, csv_files, labels, risk_scores, epochs=50, batch_size=4):
        """Entrenar el modelo con manejo de clases desbalanceadas - SOLUCIÃ“N 1"""
        print("Iniciando entrenamiento...")

        # Preparar datos
        features, labels, risk_scores = self.prepare_training_data(csv_files, labels, risk_scores)

        # Verificar distribuciÃ³n de clases
        unique_labels, counts = np.unique(labels, return_counts=True)
        num_classes = len(unique_labels)

        print(f"ğŸ¯ NÃºmero de clases: {num_classes}")
        print(f"ğŸ“Š DistribuciÃ³n de clases:")
        for label, count in zip(unique_labels, counts):
            print(f"   Clase {label}: {count} muestras")

        # SOLUCIÃ“N 1: Usar todos los datos para entrenamiento (sin validaciÃ³n)
        print("âš ï¸  Pocos datos, usando TODOS para entrenamiento (sin validaciÃ³n)")
        X_train, y_train, risk_train = features, labels, risk_scores

        print(f"ğŸ“Š Datos de entrenamiento: {len(X_train)}")

        # Convierte a tensores directamente sin DataLoader para evitar problemas de dimensiones
        X_train_tensor = torch.FloatTensor(X_train).to(self.device)
        y_train_tensor = torch.LongTensor(y_train).to(self.device)
        risk_train_tensor = torch.FloatTensor(risk_train).to(self.device)

        # Inicializar modelo
        input_dim = X_train.shape[1]

        print(f"ğŸ“ˆ Dimensiones de entrada: {input_dim}")
        print(f"ğŸ¯ NÃºmero de clases: {num_classes}")

        self.model = EnhancedDangerNeuralNetwork(input_dim, num_classes).to(self.device)

        # Definir loss functions y optimizer
        criterion_class = nn.CrossEntropyLoss()
        criterion_risk = nn.MSELoss()
        optimizer = optim.Adam(self.model.parameters(), lr=0.001, weight_decay=1e-5)

        # Entrenamiento sin DataLoader (usando todos los datos en cada Ã©poca)
        print(f"\nğŸ¯ Comenzando entrenamiento por {epochs} Ã©pocas...")

        for epoch in range(epochs):
            # Fase de entrenamiento
            self.model.train()
            optimizer.zero_grad()

            # Forward pass con todos los datos
            class_logits, risk_pred = self.model(X_train_tensor)

            # Calcular pÃ©rdidas
            loss_class = criterion_class(class_logits, y_train_tensor)
            loss_risk = criterion_risk(risk_pred.squeeze(), risk_train_tensor)
            loss = loss_class + 0.3 * loss_risk

            # Backward pass
            loss.backward()
            optimizer.step()

            # Guardar pÃ©rdida
            self.history['train_loss'].append(loss.item())

            # Mostrar progreso cada 10 Ã©pocas
            if (epoch + 1) % 10 == 0:
                print(f'Epoch {epoch + 1}/{epochs}: Loss: {loss.item():.4f}')

        self.is_trained = True
        print("âœ… Entrenamiento completado!")

        # EvaluaciÃ³n final solo con datos de entrenamiento
        self.print_comprehensive_metrics(X_train, y_train)

        # Graficar resultados
        self.plot_training_results()

    def print_comprehensive_metrics(self, X_data, y_data):
        """Imprimir mÃ©tricas completas del modelo"""
        print("\n" + "="*60)
        print("ğŸ“Š MÃ‰TRICAS COMPLETAS DEL MODELO")
        print("="*60)

        # Predicciones
        self.model.eval()
        with torch.no_grad():
            X_tensor = torch.FloatTensor(X_data).to(self.device)
            train_logits, _ = self.model(X_tensor)
            train_preds = torch.argmax(train_logits, dim=1).cpu().numpy()

        print(f"\nğŸ“ˆ Resultados en {len(X_data)} muestras:")
        print(f"Accuracy: {accuracy_score(y_data, train_preds):.4f}")
        print(f"Precision: {precision_score(y_data, train_preds, average='weighted', zero_division=0):.4f}")
        print(f"Recall: {recall_score(y_data, train_preds, average='weighted', zero_division=0):.4f}")
        print(f"F1-Score: {f1_score(y_data, train_preds, average='weighted', zero_division=0):.4f}")

        # Reporte de clasificaciÃ³n
        print(f"\nValores reales Ãºnicos: {np.unique(y_data)}")
        print(f"Predicciones Ãºnicas: {np.unique(train_preds)}")

    def plot_training_results(self):
        """Graficar resultados del entrenamiento"""
        if not self.history['train_loss']:
            print("âš ï¸  No hay datos de entrenamiento para graficar")
            return

        plt.figure(figsize=(8, 5))
        plt.plot(self.history['train_loss'], label='Training Loss', color='blue', linewidth=2)
        plt.title('PÃ©rdida de Entrenamiento')
        plt.xlabel('Ã‰poca')
        plt.ylabel('Loss')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.show()

    def predict_danger_level(self, csv_path):
        """Predecir nivel de peligro de un nuevo archivo CSV"""
        if not self.is_trained:
            raise ValueError("âŒ El modelo debe ser entrenado primero")

        try:
            # Cargar y procesar datos
            csv_data = self.load_csv_data(csv_path)
            if csv_data is None:
                return {"error": "No se pudo cargar el archivo CSV"}

            features = self.csv_extractor.extract_features(csv_data)
            if features is None:
                return {"error": "No se pudieron extraer caracterÃ­sticas"}

            # Normalizar y predecir
            features_scaled = self.scaler.transform(features.reshape(1, -1))
            features_tensor = torch.FloatTensor(features_scaled).to(self.device)

            self.model.eval()
            with torch.no_grad():
                class_logits, risk_score = self.model(features_tensor)
                probabilities = torch.softmax(class_logits, dim=1)
                predicted_class = torch.argmax(probabilities, dim=1).item()
                risk_value = risk_score.item()

            # Determinar nivel de peligro basado en el riesgo
            if risk_value > 0.7:
                danger_level = "ALTO PELIGRO"
                recommendation = "ğŸš¨ EVACUACIÃ“N INMEDIATA"
            elif risk_value > 0.5:
                danger_level = "PELIGRO MODERADO"
                recommendation = "âš ï¸ NO INGRESAR - REQUIERE INSPECCIÃ“N"
            elif risk_value > 0.3:
                danger_level = "RIESGO BAJO"
                recommendation = "ğŸ”¶ PRECAUCIÃ“N - MONITOREAR"
            else:
                danger_level = "SEGURO"
                recommendation = "âœ… ESTRUCTURA ESTABLE"

            return {
                'archivo': os.path.basename(csv_path),
                'puntuacion_riesgo': risk_value,
                'nivel_peligro': danger_level,
                'recomendacion': recommendation
            }

        except Exception as e:
            return {"error": f"Error en predicciÃ³n: {str(e)}"}

# ==================== FUNCIONES PARA CARGAR DATOS ====================
def find_all_csv_files(base_path=BASE_PATH):
    """Encontrar todos los archivos CSV en el directorio"""
    csv_files = []
    for file in os.listdir(base_path):
        if file.endswith('.csv') and file != 'metadata.csv':
            csv_files.append(os.path.join(base_path, file))
    return csv_files

def automatic_labeling(filename):
    """Etiquetado automÃ¡tico basado en nombre de archivo"""
    filename_lower = filename.lower()

    # ETIQUETAS MEJORADAS PARA SONIDOS ESTRUCTURALES
    if 'sirena' in filename_lower:
        return 'alerta_sonora', 0.8
    elif 'arma' in filename_lower or 'disparo' in filename_lower:
        return 'peligro_alto', 0.9
    elif 'explosion' in filename_lower or 'bomba' in filename_lower:
        return 'peligro_alto', 0.95
    elif 'colapso' in filename_lower or 'derrumbe' in filename_lower:
        return 'peligro_alto', 0.9
    elif 'grieta' in filename_lower or 'agrietamiento' in filename_lower:
        return 'peligro_medio', 0.7
    elif 'romp' in filename_lower or 'quebrad' in filename_lower:
        return 'peligro_medio', 0.65
    elif 'metal' in filename_lower or 'acero' in filename_lower:
        return 'metal', 0.6
    elif 'madera' in filename_lower or 'wood' in filename_lower:
        return 'madera', 0.5
    elif 'concreto' in filename_lower or 'hormigon' in filename_lower:
        return 'concreto', 0.5
    elif 'terremoto' in filename_lower or 'sismo' in filename_lower:
        return 'natural', 0.8
    elif 'helicoptero' in filename_lower or 'avion' in filename_lower:
        return 'vehiculo', 0.6
    elif 'auto' in filename_lower or 'motor' in filename_lower:
        return 'vehiculo', 0.5
    elif 'maquina' in filename_lower or 'drill' in filename_lower:
        return 'maquinaria', 0.6
    elif 'viento' in filename_lower or 'tormenta' in filename_lower:
        return 'natural', 0.4
    else:
        return 'otros', 0.3

def load_training_data_automatic():
    """Cargar datos automÃ¡ticamente con etiquetado inteligente"""
    print("ğŸ“‚ Buscando archivos CSV automÃ¡ticamente...")

    # Buscar todos los archivos CSV
    csv_files = find_all_csv_files()

    if not csv_files:
        print("âŒ No se encontraron archivos CSV")
        return [], [], []

    print(f"âœ… Encontrados {len(csv_files)} archivos CSV")

    labels = []
    risk_scores = []

    for csv_file in csv_files:
        filename = os.path.basename(csv_file)
        label, risk = automatic_labeling(filename)
        labels.append(label)
        risk_scores.append(risk)

        print(f"   {filename} -> {label} (riesgo: {risk})")

    # Mostrar distribuciÃ³n
    print("\nğŸ“Š DistribuciÃ³n automÃ¡tica de clases:")
    for label, count in Counter(labels).items():
        print(f"   {label}: {count} muestras")

    return csv_files, labels, risk_scores

# ==================== EJECUCIÃ“N PRINCIPAL ====================
print("ğŸš€ INICIANDO ENTRENAMIENTO DEL MODELO...")
print("=" * 50)

# Cargar datos automÃ¡ticamente
csv_files, labels, risk_scores = load_training_data_automatic()

if not csv_files:
    print("âŒ No hay datos para entrenar")
else:
    # Codificar etiquetas
    from sklearn.preprocessing import LabelEncoder
    label_encoder = LabelEncoder()
    labels_encoded = label_encoder.fit_transform(labels)

    print(f"ğŸ”§ CodificaciÃ³n aplicada:")
    for original, encoded in zip(labels, labels_encoded):
        print(f"   {original} -> {encoded}")

    print(f"\nğŸ“Š DistribuciÃ³n de clases:")
    class_counts = Counter(labels_encoded)
    for label, count in class_counts.items():
        print(f"   Clase {label}: {count} muestras")

    print(f"ğŸ¯ NÃºmero total de clases: {len(label_encoder.classes_)}")

    # Verificar si hay clases con solo 1 muestra
    single_sample_classes = [cls for cls, count in class_counts.items() if count == 1]
    if single_sample_classes:
        print(f"âš ï¸  Clases con solo 1 muestra: {single_sample_classes}")
        print("ğŸ“ Se usarÃ¡ validaciÃ³n manual o todos los datos para entrenamiento")

    # Entrenar el modelo
    safety_system = EnhancedStructuralSafetySystem()
    safety_system.train_model(csv_files, labels_encoded, risk_scores, epochs=50, batch_size=4)

    # Probar predicciÃ³n
    print("\nğŸ” PROBANDO PREDICCIÃ“N...")
    if csv_files:
        sample_file = csv_files[0]
        prediction = safety_system.predict_danger_level(sample_file)
        print(f"\nğŸ“‹ PREDICCIÃ“N PARA: {os.path.basename(sample_file)}")
        print("=" * 40)
        for key, value in prediction.items():
            print(f"{key:.<20}: {value}")

        print("\nğŸ‰ Â¡ENTRENAMIENTO COMPLETADO EXITOSAMENTE!")